{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6149b4c9",
   "metadata": {},
   "source": [
    "Hyperparameters will vary according to your Dataset, Its a example if your have 500 examples, 280 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train        = 500       \n",
    "batch_size     = 1\n",
    "grad_accum     = 8\n",
    "num_epochs     = 4\n",
    "steps_per_epoch= math.ceil(N_train / (batch_size * grad_accum))\n",
    "total_steps    = num_epochs * steps_per_epoch\n",
    "warmup_steps   = 10\n",
    "learning_rate  = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bae07",
   "metadata": {},
   "source": [
    "For more try this, approx 625 steps for 500 eaxmples and 5 epochs (if it overfits or overtrained then adjust, moniter the training loss \n",
    ") -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "N_train = 500 #adjust according to your dataset size\n",
    "\n",
    "per_device_batch_size = 1\n",
    "grad_accum            = 4  # reduced to allow for more gradient updates\n",
    "num_epochs            = 5  # increased to let the model see the data more times\n",
    "learning_rate         = 4e-5 # slightly higher LR can help on short training runs\n",
    "\n",
    "steps_per_epoch = math.ceil(N_train / (per_device_batch_size * grad_accum))\n",
    "max_steps       = num_epochs * steps_per_epoch\n",
    "\n",
    "# hey update SFTTrainer arguments also\n",
    "trainer_args = TrainingArguments(\n",
    "    per_device_train_batch_size = per_device_batch_size,\n",
    "    gradient_accumulation_steps = grad_accum,\n",
    "    warmup_ratio                = 0.1, \n",
    "    max_steps                   = max_steps,\n",
    "    learning_rate               = learning_rate,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    optim                       = \"adamw_8bit\",\n",
    "    fp16                        = True, \n",
    "    logging_steps               = 10,\n",
    "    save_strategy               = \"steps\",\n",
    "    save_steps                  = steps_per_epoch, # Save at the end of each epoch\n",
    "    output_dir                  = \"outputs\",\n",
    ")\n",
    "\n",
    "# and in the SFTTrainer call\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    args=trainer_args,\n",
    "    packing=True, # for efficient training\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs, which is {max_steps} total steps.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
