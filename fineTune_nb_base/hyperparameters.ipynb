{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623a1ad7",
   "metadata": {},
   "source": [
    "Look, The core training hyparams, precision,optimization,logging etc depends on your data, how much data you have and what kind of , Here for a 2k Example -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(2_000))\n",
    "N           = 2_000           # now only 2k samples\n",
    "\n",
    "batch_size  = 1\n",
    "grad_accum  = 8\n",
    "num_epochs  = 3\n",
    "steps_per_epoch = math.ceil(N / (batch_size * grad_accum))\n",
    "total_steps     = num_epochs * steps_per_epoch\n",
    "warmup_steps    = int(0.03 * total_steps)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model               = model,\n",
    "    tokenizer           = tokenizer,\n",
    "    train_dataset       = dataset[\"train\"],\n",
    "    #formatting_func     = formatting_func,\n",
    "    max_seq_length      = 512,\n",
    "    packing             = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size   = batch_size,\n",
    "        gradient_accumulation_steps   = grad_accum,\n",
    "        warmup_steps                  = warmup_steps,\n",
    "        max_steps                     = total_steps,\n",
    "        learning_rate                 = 1e-4,\n",
    "        fp16                          = not is_bfloat16_supported(),\n",
    "        bf16                          = is_bfloat16_supported(),\n",
    "        logging_steps                 = 10,\n",
    "        optim                         = \"adamw_8bit\",\n",
    "        weight_decay                  = 0.01,\n",
    "        lr_scheduler_type             = \"linear\",\n",
    "        seed                          = 3407,\n",
    "        output_dir                    = \"outputs\",\n",
    "        report_to                     = \"tensorboard\",\n",
    "        logging_dir                   = tensorboard_log_dir,\n",
    "        logging_strategy              = \"steps\",\n",
    "        save_strategy                 = \"steps\",\n",
    "        save_steps                    = steps_per_epoch,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09162d20",
   "metadata": {},
   "source": [
    "Change epochs, learning rate, batch size . max seq length etc according your data, while traning , moniter training loss and update your hyperparams according to that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc50db",
   "metadata": {},
   "source": [
    "As example for 210k rows data, only example - taskydata/baize_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import math\n",
    "\n",
    "per_device_batch_size = 2 \n",
    "grad_accum            = 16 \n",
    "num_epochs            = 1  \n",
    "max_steps = math.ceil((len(dataset[\"train\"]) / (per_device_batch_size * grad_accum)) * num_epochs)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model            = model,\n",
    "    tokenizer        = tokenizer,\n",
    "    train_dataset    = dataset[\"train\"],\n",
    "    # eval_dataset   = dataset[\"test\"], # Add this for evaluation\n",
    "    # formatting_func= formatting_func,\n",
    "    max_seq_length   = 2048, # Increased for chat data, adjust based on VRAM\n",
    "    packing          = True, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = per_device_batch_size,\n",
    "        gradient_accumulation_steps = grad_accum,\n",
    "        warmup_ratio                = 0.05, \n",
    "        max_steps                   = max_steps,\n",
    "        learning_rate               = 2e-5, \n",
    "        fp16                        = not is_bfloat16_supported(),\n",
    "        bf16                        = is_bfloat16_supported(),\n",
    "        logging_steps               = 25, # Log progress more reasonably\n",
    "        optim                       = \"adamw_8bit\",\n",
    "        weight_decay                = 0.01,\n",
    "        lr_scheduler_type           = \"cosine\", \n",
    "        seed                        = 3407,\n",
    "        output_dir                  = \"outputs\",\n",
    "        report_to                   = \"tensorboard\",\n",
    "        save_strategy               = \"steps\",\n",
    "        save_steps                  = 500, # Save checkpoints periodically\n",
    "        # evaluation_strategy       = \"steps\", # Enable evaluation\n",
    "        # eval_steps                = 500,     # Evaluate every 500 steps\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
